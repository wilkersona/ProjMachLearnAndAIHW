{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjMachL&AI_HW4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGHOADW9nTh1"
      },
      "source": [
        "Task 1:\n",
        "\n",
        "I wanted to do something related to weather, as there tends to be a wide variety of datasets to choose from, and because weather patterns are affected by what happened previously. I found a dataset with various weather information over a decade, and decided to use it to predict what temperature it would be in the next hour: \n",
        "\n",
        "[Link: https://www.kaggle.com/budincsevity/szeged-weather ]\n",
        "\n",
        "In terms of EDA, this dataset was fairly well maintained, with no missing values or weather recordings. A few data categories were unusable, such as Loud Cover (which always had a value of 0), or Daily Summary (which tended to be too specific to create any meaningful data out of). The other categories consisted of: the date and time of the recording; a summary of the individual recording (27 different values, \"Partly Cloudy\" as the most common); precipitation of rain, snow, or none; humidity from 0-1 and tending to be around 0.75; wind speed in km/h (usually around 5-10); wind direction in degrees; the visibility in km (average: 10 km); and the pressure (consistently around 1000 millibars). Most important, however, was the Temperature data, which had an average value of 11.9 degrees Celsius, and a standard deviation of 9.55 degrees (Apparent Temperature was usually 1 or 2 degrees cooler).\n",
        "\n",
        "I originally planned to use most of the data for this task, but after the first few runs of my code (which used only the Temperature data) showed extremely promising results, I opted to instead use only the Temperature data and save on runtime, as adding in the extra data would likely not improve the accuracy significantly, and could potentially lead to overfitting.\n",
        "\n",
        "In terms of what model I used, I went for TensorFlow's Keras, as it allows you to easily switch between Simple RNN layers and LSTM/GRU layers. I started by doing a train/dev/test split, then splitting the data into 24-hour segments (with overlap) so as to maximize the number of training samples the model has. I used relu activation, as the temperatures tended to not be positive and not close to 0, meaning it kept the most information when being passed through. I added one in-between layer of 10 nodes, then a final output layer of 1 node (the guessed temperature). For my loss function, I used mean squared error, as the fact that the values were not bound between 1 and 0 meant I would have to simply get as close to the actual value as possible.\n",
        "\n",
        "The link below was useful in helping me get started with my RNN, even though I eventually used a very different dataset and hyperparameters.\n",
        "\n",
        "[Link: https://www.datatechnotes.com/2018/12/rnn-example-with-keras-simplernn-in.html ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk3zPopHDj3L",
        "outputId": "b77db21e-44e9-4d7c-8115-79c782319236"
      },
      "source": [
        "# Import TensorFlow, Pandas, and Numpy\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, LSTM\n",
        "\n",
        "#https://www.datatechnotes.com/2018/12/rnn-example-with-keras-simplernn-in.html\n",
        "#https://www.kaggle.com/budincsevity/szeged-weather\n",
        "\n",
        "#read in data and print first few values\n",
        "data = pd.read_csv('weatherHistory.csv')\n",
        "\n",
        "#get rid of unneeded data\n",
        "data=data[[\"Temperature (C)\"]]\n",
        "\n",
        "#split into train/test/valid, but NOT randomly\n",
        "train,dev,test = data.values[0:80000,:], data.values[80000:90000,:], data.values[90000:len(data),:]\n",
        "\n",
        "#split data into 24-hour cycles\n",
        "def split(dat):\n",
        " X, Y =[], []\n",
        " for i in range(len(dat)-24):\n",
        "  d=i+24\n",
        "  X.append(dat[i:d,])\n",
        "  Y.append(dat[d,])\n",
        " return np.array(X), np.array(Y)\n",
        "  \n",
        "train_X,train_Y =split(train)\n",
        "dev_X,dev_Y =split(dev)\n",
        "test_X,test_Y =split(test)\n",
        "train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
        "dev_X = np.reshape(dev_X, (dev_X.shape[0], 1, dev_X.shape[1]))\n",
        "test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
        "\n",
        "#define the model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=32, input_shape=(1,24), activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"relu\")) \n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
        "model.summary()\n",
        "\n",
        "model.fit(train_X,train_Y, epochs=20, batch_size=16, verbose=1)\n",
        "\n",
        "devPredict= model.predict(dev_X)\n",
        "avg_offset=0\n",
        "for i in range(len(dev_Y)):\n",
        "  avg_offset+=pow((dev_Y[i]-devPredict[i]), 2)\n",
        "print(\"Mean Sq. Err of Dev Set: \" + str(avg_offset[0]/len(dev_Y)))\n",
        "\n",
        "testPredict= model.predict(test_X)\n",
        "avg_offset=0\n",
        "for i in range(len(test_Y)):\n",
        "  avg_offset+=pow((test_Y[i]-testPredict[i]), 2)\n",
        "print(\"Mean Sq. Err of Test Set: \" + str(avg_offset[0]/len(test_Y)))\n",
        "\n",
        "#redefine the model with LSTM\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=32, input_shape=(1,24), activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"relu\")) \n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
        "model.summary()\n",
        "\n",
        "model.fit(train_X,train_Y, epochs=20, batch_size=16, verbose=1)\n",
        "\n",
        "devPredict= model.predict(dev_X)\n",
        "avg_offset=0\n",
        "for i in range(len(dev_Y)):\n",
        "  avg_offset+=pow((dev_Y[i]-devPredict[i]), 2)\n",
        "print(\"Mean Sq. Err of Dev Set: \" + str(avg_offset[0]/len(dev_Y)))\n",
        "\n",
        "testPredict= model.predict(test_X)\n",
        "avg_offset=0\n",
        "for i in range(len(test_Y)):\n",
        "  avg_offset+=pow((test_Y[i]-testPredict[i]), 2)\n",
        "print(\"Mean Sq. Err of Test Set: \" + str(avg_offset[0]/len(test_Y)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_8 (SimpleRNN)     (None, 32)                1824      \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 2,165\n",
            "Trainable params: 2,165\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "4999/4999 [==============================] - 10s 2ms/step - loss: 2.9062\n",
            "Epoch 2/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 2.0635\n",
            "Epoch 3/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.9303\n",
            "Epoch 4/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.8800\n",
            "Epoch 5/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.8376\n",
            "Epoch 6/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.8026\n",
            "Epoch 7/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.7820\n",
            "Epoch 8/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.7610\n",
            "Epoch 9/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.7444\n",
            "Epoch 10/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.7348\n",
            "Epoch 11/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.7267\n",
            "Epoch 12/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.7203\n",
            "Epoch 13/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.7150\n",
            "Epoch 14/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.7061\n",
            "Epoch 15/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.7004\n",
            "Epoch 16/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.6977\n",
            "Epoch 17/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.6961\n",
            "Epoch 18/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.6861\n",
            "Epoch 19/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.6858\n",
            "Epoch 20/20\n",
            "4999/4999 [==============================] - 9s 2ms/step - loss: 1.6790\n",
            "Mean Sq. Err of Dev Set: 1.1067663871986384\n",
            "Mean Sq. Err of Test Set: 1.0537921152838499\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 32)                7296      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 7,637\n",
            "Trainable params: 7,637\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "4999/4999 [==============================] - 13s 2ms/step - loss: 2.6816\n",
            "Epoch 2/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.9976\n",
            "Epoch 3/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.9061\n",
            "Epoch 4/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.8518\n",
            "Epoch 5/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.8206\n",
            "Epoch 6/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.7922\n",
            "Epoch 7/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.7741\n",
            "Epoch 8/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.7493\n",
            "Epoch 9/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.7377\n",
            "Epoch 10/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.7295\n",
            "Epoch 11/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.7197\n",
            "Epoch 12/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.7118\n",
            "Epoch 13/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.6982\n",
            "Epoch 14/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.6933\n",
            "Epoch 15/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.6838\n",
            "Epoch 16/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.6770\n",
            "Epoch 17/20\n",
            "4999/4999 [==============================] - 12s 2ms/step - loss: 1.6755\n",
            "Epoch 18/20\n",
            "4999/4999 [==============================] - 12s 2ms/step - loss: 1.6686\n",
            "Epoch 19/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.6625\n",
            "Epoch 20/20\n",
            "4999/4999 [==============================] - 11s 2ms/step - loss: 1.6580\n",
            "Mean Sq. Err of Dev Set: 1.1220790572192716\n",
            "Mean Sq. Err of Test Set: 1.0942045950793617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEm2aVyptyvS"
      },
      "source": [
        "After switching to an LSTM cell-based structure, the model actually did slightly worse. This surprised me initially, until I realized the non-LSTM model likely used data from exactly 24 hours ago to inform its guess (as temperatures tend to oscillate over a 24-hour period). Because this would be placed in the long-term memory of the LSTM cell, it wasn't as reliable to retrieve, making it have an overall smaller impact on the result. However, this difference is barely noticeable, as the fact that both models can usually get within 1 or 2 degrees of the actual value is fairly accurate in my mind. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHvyH7Nwu8J7"
      },
      "source": [
        "Task 2:\n",
        "\n",
        "I opted to use a pre-trained embedding that was taken from a variety of Covid-19 news and data.\n",
        "\n",
        "[Link: https://www.tensorflow.org/hub/tutorials/cord_19_embeddings_keras ]\n",
        "\n",
        "For a dissimilarity score, I used the Euclidean distance between the two vectors. High value mean that the words are fairly far from eachother, whereas low values mean they are closer. This also comes with the added benefit of words that do not appear in the training set as often being set as far away from words that do, as there is not yet enough data to form a fully accurate guess. From my testing, it seems to work fairly well, even though there are a few flaws (it rates two words it has never seen before as very similar)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHKVDpZ4UM5N",
        "outputId": "dab190b1-da76-4d27-e295-df57cd806fe1"
      },
      "source": [
        "import functools\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tqdm import trange\n",
        "\n",
        "import math\n",
        "\n",
        "#https://www.tensorflow.org/hub/tutorials/cord_19_embeddings_keras\n",
        "\n",
        "#similarity and dissimilarity\n",
        "def correlation(word1, word2):\n",
        "  print(\"Cosine Similarity:      \", (np.inner(word1, word2)/(math.sqrt(np.inner(word1, word1))*math.sqrt(np.inner(word2, word2)))))\n",
        "  print(\"Distance Dissimilarity: \", np.inner(word1-word2, word1-word2))\n",
        "\n",
        "#load module\n",
        "module = hub.load('https://tfhub.dev/tensorflow/cord-19/swivel-128d/3')\n",
        "\n",
        "while True:\n",
        "  word1 = input(\"Enter first word (Q to quit): \")\n",
        "  if (word1==\"Q\"):\n",
        "    break\n",
        "  word2 = input(\"Enter second word:            \")\n",
        "  correlation(module([word1]), module([word2]))\n",
        "  #print(word1)\n",
        "  #print(word2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter first word (Q to quit): Spain\n",
            "Enter second word:            Italy\n",
            "Cosine Similarity:       [[0.5320724]]\n",
            "Distance Dissimilarity:  [[12.1632224]]\n",
            "Enter first word (Q to quit): SARS\n",
            "Enter second word:            MERS\n",
            "Cosine Similarity:       [[0.67628618]]\n",
            "Distance Dissimilarity:  [[5.56547353]]\n",
            "Enter first word (Q to quit): cough\n",
            "Enter second word:            fever\n",
            "Cosine Similarity:       [[0.44891569]]\n",
            "Distance Dissimilarity:  [[14.94088086]]\n",
            "Enter first word (Q to quit): Coronavirus\n",
            "Enter second word:            throat\n",
            "Cosine Similarity:       [[0.01351204]]\n",
            "Distance Dissimilarity:  [[25.09213602]]\n",
            "Enter first word (Q to quit): Europe\n",
            "Enter second word:            Europe\n",
            "Cosine Similarity:       [[1.]]\n",
            "Distance Dissimilarity:  [[0.]]\n",
            "Enter first word (Q to quit): Q\n"
          ]
        }
      ]
    }
  ]
}