{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjMachL&AI_HW5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcCqxJWQ61EP"
      },
      "source": [
        "I decided to use a dataset of different gemstones (87 categories) I found, partly because the images just look pretty. Unfortunately, this dataset only had a train/test split instead of train/test/dev, so I was unable to score how well my model generalized. The dataset was also a bit unbalanced, and used images of non-uniform size.\n",
        "\n",
        "[Link: https://www.kaggle.com/lsind18/gemstones-images]\n",
        "\n",
        "I got some coding help from the following sites, but the majority of code is original (except the GoogLeNet):\n",
        "\n",
        "[Link: https://www.tensorflow.org/tutorials/images/cnn]\n",
        "\n",
        "[GoogLeNet: https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/]\n",
        "\n",
        "For the Part 1 model, I decided to alternate Convolution and Max Pooling layers, as I have done the same when building CNNs in the past, and it has worked relatively well. I used a 3x3 filter for convolution, and a 2x2 for Max Pooling, while increasing the depth to 32 and 64, in order to retain the most local information about the image between layers. Finally, I repeated this three times total, to gain the most information out of the image as possible.\n",
        "\n",
        "In Part 2, I implemented GoogLeNet (I had a lot of help), but changed certain variables to match my dataset more accurately. Unfortunately, I had to drastically decrease the number of epochs because the runtime was simply abhorrent, and I wanted to actually be able to turn this assignment in.\n",
        "\n",
        "In Part 3, I was able to rotate the images and flip them both horizontally and vertically (while loading them in the ImageDataGenerator), because gemstones in particular don't exactly need a sense of orientation to be a type of gemstone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m1BFprk4dax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b9b92d-c19c-4b74-f424-af07dd239fd6"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, MaxPool2D, GlobalAveragePooling2D, AveragePooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense, Input, concatenate\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!unzip archive\\ \\(2\\).zip\n",
        "\n",
        "train_data_dir = \"train\"\n",
        "test_data_dir = \"test\"\n",
        "img_width = 255\n",
        "img_height= 255\n",
        "batch_size = 1\n",
        "\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "                rescale = 1. / 255,\n",
        "                 shear_range = 0.2,\n",
        "                  zoom_range = 0.2,\n",
        "                  rotation_range = 360,\n",
        "                  vertical_flip=True,\n",
        "            horizontal_flip = True)\n",
        "  \n",
        "test_datagen = ImageDataGenerator(rescale = 1. / 255)\n",
        "  \n",
        "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
        "                              target_size =(img_width, img_height), color_mode = \"rgb\",\n",
        "                     batch_size = 100, class_mode ='categorical')\n",
        "  \n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "                                    test_data_dir,\n",
        "                   target_size =(img_width, img_height), color_mode = \"rgb\",\n",
        "          batch_size = 33, class_mode ='categorical')\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "class_names = ['Alexandrite', 'Almandine', 'Amazonite', 'Amber', 'Amethyst',\n",
        "               'Ametrine', 'Andalusite', 'Andradite', 'Aquamarine', 'Aventurine Green',\n",
        "               'Aventurine Yellow', 'Benitoite', 'Beryl Golden', 'Bixbite', 'Bloodstone',\n",
        "               'Blue Lace Agate', 'Carnelian', 'Cats Eye', 'Chalcedony', 'Chalcedony Blue',\n",
        "               'Chrome Diopside', 'Chrysoberyl', 'Chrysocolla', 'Chrysoprase', 'Citrine',\n",
        "               'Coral', 'Danburite', 'Diamond', 'Diaspore', 'Dumortierite',\n",
        "               'Emerald', 'Fluorite', 'Garnet Red', 'Goshenite', 'Grossular',\n",
        "               'Hessonite', 'Hiddenite', 'Iolite', 'Jade', 'Jasper',\n",
        "               'Kunzite', 'Kyanite', 'Labradorite', 'Lapis Lazuli', 'Larimar', \n",
        "               'Malachite', 'Moonstone', 'Morganite', 'Onyx Black', 'Onyx Green', \n",
        "               'Onyx Red', 'Opal', 'Pearl', 'Peridot', 'Prehnite', \n",
        "               'Pyrite', 'Pyrope', 'Quartz Beer', 'Quartz Lemon', 'Quartz Rose', \n",
        "               'Quartz Rutilated', 'Quartz Smoky', 'Rhodochrosite', 'Rhodolite', 'Rhodonite', \n",
        "               'Ruby', 'Sapphire Blue', 'Sapphire Pink', 'Sapphire Purple', 'Sapphire Yellow', \n",
        "               'Scapolite', 'Serpentine', 'Sodalite', 'Spassartite', 'Sphene', \n",
        "               'Spinel', 'Spodumene', 'Sunstone', 'Tanzanite', 'Tigers Eye', \n",
        "               'Topaz', 'Tourmaline', 'Tsavorite','Turquoise', 'Variscite', \n",
        "               'Zircon', 'Zoisite']\n",
        "\n",
        "'''\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i])\n",
        "    # The CIFAR labels happen to be arrays, \n",
        "    # which is why you need the extra index\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "#Base Model-----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(255, 255, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(87))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss ='categorical_crossentropy',\n",
        "                     optimizer ='rmsprop',\n",
        "                   metrics =['categorical_accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "'''\n",
        "model.fit(train_generator,\n",
        "      steps_per_epoch = 28,\n",
        "      epochs = 10, validation_data = test_generator,\n",
        "      validation_steps = 11)\n",
        "'''\n",
        "\n",
        "#GoogLeNet-----------------------------------------------------------------------------------\n",
        "def inception_module(x,\n",
        "                     filters_1x1,\n",
        "                     filters_3x3_reduce,\n",
        "                     filters_3x3,\n",
        "                     filters_5x5_reduce,\n",
        "                     filters_5x5,\n",
        "                     filters_pool_proj,\n",
        "                     name=None):\n",
        "    \n",
        "    conv_1x1 = Conv2D(filters_1x1, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
        "    \n",
        "    conv_3x3 = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
        "    conv_3x3 = Conv2D(filters_3x3, (3, 3), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_3x3)\n",
        "\n",
        "    conv_5x5 = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
        "    conv_5x5 = Conv2D(filters_5x5, (5, 5), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_5x5)\n",
        "\n",
        "    pool_proj = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n",
        "    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(pool_proj)\n",
        "\n",
        "    output = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)\n",
        "    \n",
        "    return output\n",
        "\n",
        "kernel_init = keras.initializers.glorot_uniform()\n",
        "bias_init = keras.initializers.Constant(value=0.2)\n",
        "\n",
        "input_layer = Input(shape=(224, 224, 3))\n",
        "\n",
        "x = Conv2D(64, (7, 7), padding='same', strides=(2, 2), activation='relu', name='conv_1_7x7/2', kernel_initializer=kernel_init, bias_initializer=bias_init)(input_layer)\n",
        "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_1_3x3/2')(x)\n",
        "x = Conv2D(64, (1, 1), padding='same', strides=(1, 1), activation='relu', name='conv_2a_3x3/1')(x)\n",
        "x = Conv2D(192, (3, 3), padding='same', strides=(1, 1), activation='relu', name='conv_2b_3x3/1')(x)\n",
        "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_2_3x3/2')(x)\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=64,\n",
        "                     filters_3x3_reduce=96,\n",
        "                     filters_3x3=128,\n",
        "                     filters_5x5_reduce=16,\n",
        "                     filters_5x5=32,\n",
        "                     filters_pool_proj=32,\n",
        "                     name='inception_3a')\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=128,\n",
        "                     filters_3x3_reduce=128,\n",
        "                     filters_3x3=192,\n",
        "                     filters_5x5_reduce=32,\n",
        "                     filters_5x5=96,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_3b')\n",
        "\n",
        "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_3_3x3/2')(x)\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=192,\n",
        "                     filters_3x3_reduce=96,\n",
        "                     filters_3x3=208,\n",
        "                     filters_5x5_reduce=16,\n",
        "                     filters_5x5=48,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_4a')\n",
        "\n",
        "\n",
        "x1 = AveragePooling2D((5, 5), strides=3)(x)\n",
        "x1 = Conv2D(128, (1, 1), padding='same', activation='relu')(x1)\n",
        "x1 = Flatten()(x1)\n",
        "x1 = Dense(1024, activation='relu')(x1)\n",
        "x1 = Dropout(0.7)(x1)\n",
        "x1 = Dense(87, activation='softmax', name='auxilliary_output_1')(x1)\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=160,\n",
        "                     filters_3x3_reduce=112,\n",
        "                     filters_3x3=224,\n",
        "                     filters_5x5_reduce=24,\n",
        "                     filters_5x5=64,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_4b')\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=128,\n",
        "                     filters_3x3_reduce=128,\n",
        "                     filters_3x3=256,\n",
        "                     filters_5x5_reduce=24,\n",
        "                     filters_5x5=64,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_4c')\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=112,\n",
        "                     filters_3x3_reduce=144,\n",
        "                     filters_3x3=288,\n",
        "                     filters_5x5_reduce=32,\n",
        "                     filters_5x5=64,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_4d')\n",
        "\n",
        "\n",
        "x2 = AveragePooling2D((5, 5), strides=3)(x)\n",
        "x2 = Conv2D(128, (1, 1), padding='same', activation='relu')(x2)\n",
        "x2 = Flatten()(x2)\n",
        "x2 = Dense(1024, activation='relu')(x2)\n",
        "x2 = Dropout(0.7)(x2)\n",
        "x2 = Dense(87, activation='softmax', name='auxilliary_output_2')(x2)\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=256,\n",
        "                     filters_3x3_reduce=160,\n",
        "                     filters_3x3=320,\n",
        "                     filters_5x5_reduce=32,\n",
        "                     filters_5x5=128,\n",
        "                     filters_pool_proj=128,\n",
        "                     name='inception_4e')\n",
        "\n",
        "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_4_3x3/2')(x)\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=256,\n",
        "                     filters_3x3_reduce=160,\n",
        "                     filters_3x3=320,\n",
        "                     filters_5x5_reduce=32,\n",
        "                     filters_5x5=128,\n",
        "                     filters_pool_proj=128,\n",
        "                     name='inception_5a')\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=384,\n",
        "                     filters_3x3_reduce=192,\n",
        "                     filters_3x3=384,\n",
        "                     filters_5x5_reduce=48,\n",
        "                     filters_5x5=128,\n",
        "                     filters_pool_proj=128,\n",
        "                     name='inception_5b')\n",
        "\n",
        "x = GlobalAveragePooling2D(name='avg_pool_5_3x3/1')(x)\n",
        "\n",
        "x = Dropout(0.4)(x)\n",
        "\n",
        "x = Dense(87, activation='softmax', name='output')(x)\n",
        "\n",
        "model = Model(input_layer, [x, x1, x2], name='inception_v1')\n",
        "\n",
        "epochs = 4\n",
        "initial_lrate = 0.01\n",
        "\n",
        "def decay(epoch, steps=100):\n",
        "    initial_lrate = 0.01\n",
        "    drop = 0.96\n",
        "    epochs_drop = 8\n",
        "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "    return lrate\n",
        "\n",
        "#sgd = SGD(lr=initial_lrate, momentum=0.9, nesterov=False)\n",
        "\n",
        "lr_sc = LearningRateScheduler(decay, verbose=1)\n",
        "\n",
        "model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'], loss_weights=[1, 0.3, 0.3],optimizer ='rmsprop',metrics =['categorical_accuracy'])\n",
        "\n",
        "model.fit(train_generator,\n",
        "      steps_per_epoch = 28,\n",
        "      epochs = 4, validation_data = test_generator,\n",
        "      validation_steps = 11)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  archive (2).zip\n",
            "replace test/Alexandrite/alexandrite_18.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "Found 2856 images belonging to 87 classes.\n",
            "Found 363 images belonging to 87 classes.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 253, 253, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 126, 126, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 124, 124, 32)      9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 62, 62, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 60, 60, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 30, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 57600)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               7372928   \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 87)                11223     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 87)                0         \n",
            "=================================================================\n",
            "Total params: 7,412,791\n",
            "Trainable params: 7,412,791\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "28/28 [==============================] - 840s 30s/step - loss: 10.1407 - output_loss: 7.1899 - auxilliary_output_1_loss: 4.8436 - auxilliary_output_2_loss: 4.9926 - output_categorical_accuracy: 0.0120 - auxilliary_output_1_categorical_accuracy: 0.0087 - auxilliary_output_2_categorical_accuracy: 0.0131 - val_loss: 7.1765 - val_output_loss: 4.4949 - val_auxilliary_output_1_loss: 4.4661 - val_auxilliary_output_2_loss: 4.4726 - val_output_categorical_accuracy: 0.0110 - val_auxilliary_output_1_categorical_accuracy: 0.0138 - val_auxilliary_output_2_categorical_accuracy: 0.0138\n",
            "Epoch 2/4\n",
            "28/28 [==============================] - 835s 30s/step - loss: 7.2159 - output_loss: 4.5326 - auxilliary_output_1_loss: 4.4722 - auxilliary_output_2_loss: 4.4724 - output_categorical_accuracy: 0.0069 - auxilliary_output_1_categorical_accuracy: 0.0160 - auxilliary_output_2_categorical_accuracy: 0.0083 - val_loss: 7.1592 - val_output_loss: 4.4801 - val_auxilliary_output_1_loss: 4.4653 - val_auxilliary_output_2_loss: 4.4650 - val_output_categorical_accuracy: 0.0138 - val_auxilliary_output_1_categorical_accuracy: 0.0138 - val_auxilliary_output_2_categorical_accuracy: 0.0138\n",
            "Epoch 3/4\n",
            "28/28 [==============================] - 831s 30s/step - loss: 7.1926 - output_loss: 4.5096 - auxilliary_output_1_loss: 4.4716 - auxilliary_output_2_loss: 4.4716 - output_categorical_accuracy: 0.0134 - auxilliary_output_1_categorical_accuracy: 0.0145 - auxilliary_output_2_categorical_accuracy: 0.0123 - val_loss: 7.1588 - val_output_loss: 4.4788 - val_auxilliary_output_1_loss: 4.4683 - val_auxilliary_output_2_loss: 4.4651 - val_output_categorical_accuracy: 0.0138 - val_auxilliary_output_1_categorical_accuracy: 0.0138 - val_auxilliary_output_2_categorical_accuracy: 0.0138\n",
            "Epoch 4/4\n",
            "28/28 [==============================] - 817s 29s/step - loss: 7.1823 - output_loss: 4.4951 - auxilliary_output_1_loss: 4.4856 - auxilliary_output_2_loss: 4.4719 - output_categorical_accuracy: 0.0120 - auxilliary_output_1_categorical_accuracy: 0.0181 - auxilliary_output_2_categorical_accuracy: 0.0134 - val_loss: 7.1557 - val_output_loss: 4.4765 - val_auxilliary_output_1_loss: 4.4662 - val_auxilliary_output_2_loss: 4.4646 - val_output_categorical_accuracy: 0.0138 - val_auxilliary_output_1_categorical_accuracy: 0.0165 - val_auxilliary_output_2_categorical_accuracy: 0.0055\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5b4ff54b50>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgQsJSUer9s0"
      },
      "source": [
        "Task 1 Results:\n",
        "\n",
        "The first network performed reasonably well, earning a 42% categorical accuracy score in around 35 minutes.\n",
        "\n",
        "Inception V3 actually performed surprisingly poorly. It took a full hour to run just four epochs, and by the time it had completed, none of its output accuracies came even close to the first CNN, including when the first CNN was at its fourth epoch. Usually when a traditionally 'better' model performs worse, I am able to find some small area that could explain the difference. However, the difference here is significant that I am positively baffled.\n",
        "\n",
        "Making the images rotate and flip earned a 43% categorical accuracy score in around 37 minutes. This is not too far from the first network, which makes sense as they used the same model. The data augmentation step seemed to trade off accuracy for runtime, but in the real world I would imagine it would generalize much better due to the fact that it understands the gemstones from multiple 'angles'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5VLHD31xrHh"
      },
      "source": [
        "For Task 2, I found a dataset of Bob Ross paintings, and thought it would be cool to see if a VAE or GAN could replicate them. Unfortunately, at this point I pretty much out of time and will likely not finish this part of the task. I read through and understood how to implement both VAEs and GANs (I had ample time while Task 1 was running for hours), and if I had more time I would likely only need to copy-paste large portions of code, then tune hyperparameters. When I originally found the dataset, I got really excited for this portion of the homework (I might even do it just for fun after submitting this), but I simply do not have enough time to get my code running, much less wait for the training. At least I learned to never underestimate the time it takes to train a model!\n",
        "\n",
        "[Link: https://www.kaggle.com/residentmario/segmented-bob-ross-images]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "W3lVbT59wmtA",
        "outputId": "da26f77e-abe8-4f4f-f557-7b7059afac49"
      },
      "source": [
        "from IPython import display\n",
        "\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import time\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "!unzip archive\\ \\(3\\).zip\n",
        "\n",
        "def preprocess_images(images):\n",
        "  images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\n",
        "  return np.where(images > .5, 1.0, 0.0).astype('float32')\n",
        "\n",
        "images = []\n",
        "for f in glob.iglob(\"train/images/*\"):\n",
        "    images.append(np.asarray(Image.open(f)))\n",
        "\n",
        "images = np.array(images)\n",
        "\n",
        "\n",
        "train_images = preprocess_images(images)\n",
        "test_images = preprocess_images(test_images)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  archive (3).zip\n",
            "replace labels.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a214942ec88f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mtest_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-a214942ec88f>\u001b[0m in \u001b[0;36mpreprocess_images\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 113737500 into shape (250,28,28,1)"
          ]
        }
      ]
    }
  ]
}