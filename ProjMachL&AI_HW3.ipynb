{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjMachL&AI_HW3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8hSOYj3ZKjC"
      },
      "source": [
        "Problem 1.)\n",
        "I chose to use a diamonds dataset from Kaggle, and create my NN in tensorflow. \n",
        "\n",
        "[Link: https://www.kaggle.com/shivam2503/diamonds ]\n",
        "\n",
        "This dataset contains a variety of information for a series of diamonds, as well as their price, which is what I aim to guess. I did need to clean the data a bit, mostly because some of the values were strings instead of numbers (which is helpful since I didn't lose any data). Additionally, since I wanted to do a binary classification task, I decided to compare whether the price of the diamond was over or under $4,000 (approximately the average).\n",
        "\n",
        "The main tools I used were Gradient Tape in Tensorflow to record and calculate the gradients at every step in my algorithm, and the Adams Optimizer. I experimented with a wide variety of other tools, but these two are ultimately what got my project working.\n",
        "\n",
        "Gradient Tape essentially works by remembering the state of a few variables (in our case, the weights and biases), and then calculating the derivative every time they were changed. This was an absolute necessity in making back propogation efficient, and streamlined my code. Unfortunately, this did mean I was unable to use my own custom loss algorithm, as the input to my algorithm would need to be casted (which is non-differentiable) to another variable type.\n",
        "\n",
        "[Tape: https://www.tensorflow.org/api_docs/python/tf/GradientTape ]\n",
        "\n",
        "I opted to use the Adams Optimizer, mainly because I wanted to challenge myself. It also helped make my NN more efficient and accurate, which definitely sped up the process of hyperparameter tuning.\n",
        "\n",
        "[Adams: https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/AdamOptimizer?hl=ko ]\n",
        "\n",
        "I should mention that a portion of my code used concepts from the website below. However, due to the fact that my dataset was vastly different, I needed to personalize a lot of the code, such as the layer sizes, activation functions, and dev set comparisons. This article is still, however, very informative and was a great tool while debugging.\n",
        "\n",
        "[Link: https://adventuresinmachinelearning.com/python-tensorflow-tutorial/ ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxMV0v3Mw7Qw",
        "outputId": "84c1f11e-9c8e-431f-86b3-77eecc67500b"
      },
      "source": [
        "# Import TensorFlow, Pandas, and Numpy\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "#read in data and print first few values\n",
        "data = pd.read_csv('diamonds.csv')\n",
        "\n",
        "def priceGEQ (price):\n",
        "  if price > 4000:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "  \n",
        "\n",
        "#https://www.geeksforgeeks.org/replacing-strings-with-numbers-in-python-for-data-analysis/\n",
        "cutNums = {\"Fair\": 1, \"Good\": 2, \"Very Good\": 3, \"Premium\": 4, \"Ideal\": 5}\n",
        "data['cutNums'] = [cutNums[item] for item in data['cut']]\n",
        "colorNums = {\"J\": 1, \"I\": 2, \"H\": 3, \"G\": 4, \"F\": 5, \"E\": 6, \"D\": 7}\n",
        "data['colorNums'] = [colorNums[item] for item in data['color']]\n",
        "clarityNums = {\"I3\": 1, \"I2\": 2, \"I1\": 3, \"SI2\": 4, \"SI1\": 5, \"VS2\": 6, \"VS1\": 7, \"VVS2\": 8, \"VVS1\": 9, \"IF\": 10, \"FL\": 11}\n",
        "data['clarityNums'] = [clarityNums[item] for item in data['clarity']]\n",
        "data['priceBools'] = [priceGEQ(item) for item in data['price']]\n",
        "data = data.drop('Unnamed: 0',axis=1)\n",
        "data = data.drop('cut',axis=1)\n",
        "data = data.drop('color',axis=1)\n",
        "data = data.drop('clarity',axis=1)\n",
        "data = data.drop('price',axis=1)\n",
        "\n",
        "#split into training, dev, and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = data.drop('priceBools',axis=1)\n",
        "Y = data[['priceBools']]\n",
        "X_new, X_test, Y_new, Y_test = train_test_split(X, Y, test_size = 0.02,random_state=0)\n",
        "X_train, X_dev, Y_train, Y_dev = train_test_split(X_new, Y_new, test_size = 0.1,random_state=0)\n",
        "\n",
        "#https://www.geeksforgeeks.org/deep-neural-net-with-forward-and-back-propagation-from-scratch-python/\n",
        "#https://adventuresinmachinelearning.com/python-tensorflow-tutorial/\n",
        "\n",
        "#Hyperparameters\n",
        "numHiddenNodes = 10\n",
        "batch_size = 300\n",
        "num_epochs = 30\n",
        "\n",
        "# now declare the weights connecting the input to the hidden layer\n",
        "W1 = tf.Variable(tf.random.normal([9, numHiddenNodes], stddev=0.03), name='W1')\n",
        "b1 = tf.Variable(tf.random.normal([numHiddenNodes]), name='b1')\n",
        "# and the weights connecting the hidden layer to the output layer\n",
        "W2 = tf.Variable(tf.random.normal([numHiddenNodes, 1], stddev=0.03), name='W2')\n",
        "b2 = tf.Variable(tf.random.normal([1]), name='b2')\n",
        "\n",
        "#https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/\n",
        "# function to create a list containing mini-batches\n",
        "def create_mini_batches(X, y, batch_size):\n",
        "    mini_batches = []\n",
        "    data = np.hstack((X, y))\n",
        "    np.random.shuffle(data)\n",
        "    n_minibatches = data.shape[0] // batch_size\n",
        "    i = 0\n",
        "  \n",
        "    for i in range(n_minibatches + 1):\n",
        "        mini_batch = data[i * batch_size:(i + 1)*batch_size, :]\n",
        "        X_mini = mini_batch[:, :-1]\n",
        "        Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
        "        mini_batches.append((X_mini, Y_mini))\n",
        "    if data.shape[0] % batch_size != 0:\n",
        "        mini_batch = data[i * batch_size:data.shape[0]]\n",
        "        X_mini = mini_batch[:, :-1]\n",
        "        Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
        "        mini_batches.append((X_mini, Y_mini))\n",
        "    return mini_batches\n",
        "\n",
        "def forward_prop (X, W1, b1, W2, b2):\n",
        "    Z1 = tf.add(tf.matmul(tf.cast(X, tf.float32), W1), b1)\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    Z2 = tf.add(tf.matmul(A1, W2), b2)\n",
        "    A2 = tf.nn.sigmoid(Z2)\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "def cross_entropy (A, Y):\n",
        "  #add 0.00000000001 so it doesn't crash when A=0\n",
        "  logs = np.multiply(np.log(A + 0.0000000001), Y) + np.multiply((1 - Y), np.log(1.000000000001 - A))\n",
        "  return tf.reduce_mean(logs)\n",
        "\n",
        "def loss_fn(logits, labels):\n",
        "    cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels,\n",
        "                                                                              logits=logits))\n",
        "    return cross_entropy\n",
        "\n",
        "# setup the optimizer\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "total_batch = int(len(Y_train) / batch_size)\n",
        "for epoch in range(num_epochs):\n",
        "    avg_loss = 0\n",
        "    my_batches = create_mini_batches(X_train, Y_train, batch_size)\n",
        "    for i in range(total_batch):\n",
        "        batch_x, batch_y = my_batches[i]\n",
        "        # create tensors\n",
        "        batch_x = tf.cast(tf.Variable(batch_x), tf.float32)\n",
        "        batch_y = tf.cast(tf.Variable(batch_y), tf.int32)\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch([W1, b1, W2, b2])\n",
        "            Z1, A1, Z2, A2 = forward_prop(batch_x, W1, b1, W2, b2)\n",
        "            #loss = cross_entropy(A2, batch_y)\n",
        "            loss = loss_fn(Z2, tf.cast(batch_y, tf.float32))\n",
        "        gradients = tape.gradient(loss, [W1, b1, W2, b2])\n",
        "        #gradients = optimizer.compute_gradients(tf.fill([1], loss), [W1, b1, W2, b2])\n",
        "        optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2]))\n",
        "        avg_loss += loss / total_batch\n",
        "    trash1, trash2, trash3, test_logits = forward_prop(X_dev, W1, b1, W2, b2)\n",
        "    max_idxs = [item[0]>0.5 for item in test_logits]\n",
        "    test_acc = np.sum(np.array(max_idxs) == Y_dev['priceBools']) / len(Y_dev)\n",
        "    print(f\"Epoch: {epoch + 1}, loss={avg_loss:.3f}, test set      accuracy={test_acc*100:.3f}%\")\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "#test set\n",
        "trash1, trash2, trash3, test_logits = forward_prop(X_test, W1, b1, W2, b2)\n",
        "max_idxs = [item[0]>0.5 for item in test_logits]\n",
        "test_acc = np.sum(np.array(max_idxs) == Y_test['priceBools']) / len(Y_test)\n",
        "print(f\"Final test accuracy={test_acc*100:.3f}%\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, loss=0.637, test set      accuracy=64.782%\n",
            "Epoch: 2, loss=0.497, test set      accuracy=84.207%\n",
            "Epoch: 3, loss=0.378, test set      accuracy=89.408%\n",
            "Epoch: 4, loss=0.309, test set      accuracy=89.938%\n",
            "Epoch: 5, loss=0.265, test set      accuracy=94.496%\n",
            "Epoch: 6, loss=0.235, test set      accuracy=94.250%\n",
            "Epoch: 7, loss=0.213, test set      accuracy=94.912%\n",
            "Epoch: 8, loss=0.197, test set      accuracy=95.877%\n",
            "Epoch: 9, loss=0.184, test set      accuracy=95.158%\n",
            "Epoch: 10, loss=0.173, test set      accuracy=95.725%\n",
            "Epoch: 11, loss=0.164, test set      accuracy=95.423%\n",
            "Epoch: 12, loss=0.156, test set      accuracy=95.915%\n",
            "Epoch: 13, loss=0.150, test set      accuracy=95.952%\n",
            "Epoch: 14, loss=0.145, test set      accuracy=96.104%\n",
            "Epoch: 15, loss=0.140, test set      accuracy=95.688%\n",
            "Epoch: 16, loss=0.136, test set      accuracy=96.066%\n",
            "Epoch: 17, loss=0.133, test set      accuracy=96.066%\n",
            "Epoch: 18, loss=0.129, test set      accuracy=96.123%\n",
            "Epoch: 19, loss=0.127, test set      accuracy=96.047%\n",
            "Epoch: 20, loss=0.124, test set      accuracy=96.104%\n",
            "Epoch: 21, loss=0.122, test set      accuracy=95.915%\n",
            "Epoch: 22, loss=0.121, test set      accuracy=96.179%\n",
            "Epoch: 23, loss=0.119, test set      accuracy=96.293%\n",
            "Epoch: 24, loss=0.117, test set      accuracy=95.631%\n",
            "Epoch: 25, loss=0.117, test set      accuracy=96.236%\n",
            "Epoch: 26, loss=0.115, test set      accuracy=96.160%\n",
            "Epoch: 27, loss=0.114, test set      accuracy=96.179%\n",
            "Epoch: 28, loss=0.113, test set      accuracy=96.123%\n",
            "Epoch: 29, loss=0.111, test set      accuracy=96.312%\n",
            "Epoch: 30, loss=0.110, test set      accuracy=96.179%\n",
            "\n",
            "Training complete!\n",
            "Final test accuracy=96.478%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAWz4Eknfdgi"
      },
      "source": [
        "Problem 3.) \n",
        "\n",
        "As soon as my NN was working, it was already fairly accurate. My choice to use relu, then sigmoid activation helped a lot, as the activation functions were chosen with my dataset in mind. From there, I grouped the three other main hyperparameters together (number of hidden nodes, batch size, number of epochs) and changed them one-by-onetrying to get the best combination of accuracy and runtime. Interestingly, before I increased the number of hidden nodes, the accuracy used to stagnate for the first few epochs, then make a huge jump upwards before proceeding normally. I chose not to use regularizaton, as this project had already given me enough of a headache. I did, however, use the Adams Optimizer, which most definitely paid off. The values I eventually reached seem pretty stable, and appear to generalize well."
      ]
    }
  ]
}